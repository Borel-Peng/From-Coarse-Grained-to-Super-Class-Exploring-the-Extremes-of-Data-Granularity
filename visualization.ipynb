{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction and t-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from dataprocessor.datacomposer import getData\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "import clip\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from sklearn.manifold import TSNE\n",
    "import hyperparameters as HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print(\"CUDA_VISIBLE_DEVICES: \", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CLIP model\n",
    "model, _ = clip.load(HP.clip_model, device=device)\n",
    "\n",
    "# Define transforms for image processing\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Resize(256),\n",
    "    CenterCrop(224),\n",
    "    Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "              (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "\n",
    "# Load CIFAR10 test dataset\n",
    "cifar_test_dataset = CIFAR10(\n",
    "    root='data/', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features using CLIP model\n",
    "X = []\n",
    "y = []\n",
    "for i in tqdm(range(len(cifar_test_dataset))):\n",
    "    img, label = cifar_test_dataset[i]\n",
    "    img = img.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(img)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    X.append(image_features.cpu().numpy())\n",
    "    y.append(label)\n",
    "X = np.concatenate(X, axis=0)\n",
    "y = np.array(y)\n",
    "\n",
    "# Perform t-SNE dimensionality reduction\n",
    "X_embedded = TSNE(n_components=2, learning_rate=100).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR10 class information\n",
    "\n",
    "0: airplane, 1: automobile, 2: bird, 3: cat, 4: deer\n",
    "5: dog, 6: frog, 7: horse, 8: ship, 9: truck\n",
    "\n",
    "Define color map for visualization\n",
    "\n",
    "Group classes into two categories:\n",
    "\n",
    "Artificial objects (0, 1, 8, 9) and animals (2, 3, 4, 5, 6, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = {0: '#03045e', 1: '#023e8a', 8: '#0077b6', 9: '#00b4d8',\n",
    "        2: '#7a0103', 3: '#8e0103', 4: '#a50104', 5: '#b81702', 6: '#eb1d1d', 7: '#f72634'}\n",
    "\n",
    "# Visualize the t-SNE results\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(10):\n",
    "    indices = y == i\n",
    "    plt.scatter(X_embedded[indices, 0],\n",
    "                X_embedded[indices, 1], c=cmap[i], label=i, alpha=0.6)\n",
    "plt.legend()\n",
    "plt.savefig('tsne_visualization_{}.png'.format(HP.clip_model))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use SCLRE to visualize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperparameters as HP\n",
    "from model.TC2 import TransformerContrastive\n",
    "from model.TC2 import get_backbone\n",
    "HP.cls_num = 2\n",
    "model_path = ''\n",
    "checkpoint = torch.load(model_path)  # Path to the checkpoint file\n",
    "model = TransformerContrastive()\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(cifar_test_dataset, batch_size=64, shuffle=False)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "        image_features, _ = model(images)\n",
    "\n",
    "        image_features = image_features / \\\n",
    "            image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        X.append(image_features.cpu().numpy())\n",
    "        y.append(labels.numpy())\n",
    "X = np.concatenate(X, axis=0)\n",
    "y = np.concatenate(y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, learning_rate=100, random_state=42)\n",
    "X_embedded = tsne.fit_transform(X)\n",
    "cmap = {0: '#03045e', 1: '#023e8a', 8: '#0077b6', 9: '#00b4d8',\n",
    "        2: '#7a0103', 3: '#8e0103', 4: '#a50104', 5: '#b81702', 6: '#eb1d1d', 7: '#f72634'}\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(X_embedded.shape[0]):\n",
    "    plt.scatter(X_embedded[i, 0], X_embedded[i, 1],\n",
    "                c=cmap[y[i]], edgecolors='none', s=10, alpha=0.7)\n",
    "plt.axis('off')\n",
    "plt.savefig('tsne_on_{}.pdf'.format(model_path), dpi=500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bingxing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
